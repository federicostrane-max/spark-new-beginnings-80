# Pipeline B - Complete Document Processing System

## Overview

Pipeline B is a **completely independent** document processing system, designed from scratch to replace the fragile existing pipeline. It uses a multi-stage asynchronous architecture with Landing AI for chunking and OpenAI for embeddings.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PIPELINE B FLOW                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STAGE 1: INGEST (Immediate, <100ms)
â”œâ”€â”€ pipeline-b-ingest-pdf
â”‚   â””â”€â”€ Upload PDF â†’ Storage
â”‚   â””â”€â”€ Create record in pipeline_b_documents (status: 'ingested')
â”‚
â””â”€â”€ pipeline-b-ingest-github
    â””â”€â”€ Fetch files from GitHub
    â””â”€â”€ Store full_text in pipeline_b_documents (status: 'ingested')

                        â†“

STAGE 2: PROCESS CHUNKS (Async, CRON every 10 min)
â””â”€â”€ pipeline-b-process-chunks
    â”œâ”€â”€ Fetch 'ingested' documents (max 5 per run)
    â”œâ”€â”€ Call Landing AI parse API
    â”œâ”€â”€ Store chunks in pipeline_b_chunks_raw
    â”‚   â””â”€â”€ embedding_status: 'pending'
    â””â”€â”€ Update document status: 'chunked'

                        â†“

STAGE 3: GENERATE EMBEDDINGS (Async, CRON every 5 min)
â””â”€â”€ pipeline-b-generate-embeddings
    â”œâ”€â”€ Fetch 'pending' chunks (max 50 per run)
    â”œâ”€â”€ Generate OpenAI embeddings (batch)
    â”œâ”€â”€ Update chunks with embedding vector
    â””â”€â”€ Set embedding_status: 'ready'

                        â†“

STAGE 4: AGENT SYNC (On-Demand)
â””â”€â”€ pipeline-b-sync-agent
    â”œâ”€â”€ Fetch 'ready' chunks for specific documents
    â”œâ”€â”€ Create links in pipeline_b_agent_knowledge
    â””â”€â”€ Agent now has access to knowledge base
```

## Database Schema

### Tables

#### `pipeline_b_documents`
Primary documents table. Each row = one uploaded/imported document.

```sql
- id (UUID, PK)
- source_type ('pdf' | 'github' | 'markdown' | 'text')
- file_name (TEXT)
- file_path (TEXT) -- Storage path for PDFs
- storage_bucket (TEXT) -- 'knowledge-pdfs'
- full_text (TEXT) -- For GitHub/text files
- repo_url (TEXT) -- GitHub repo
- repo_path (TEXT) -- Path in repo
- file_size_bytes (INT)
- page_count (INT)
- status ('ingested' | 'processing' | 'chunked' | 'failed')
- error_message (TEXT)
- created_at, updated_at, processed_at (TIMESTAMPTZ)
```

**Status Flow**: `ingested` â†’ `processing` â†’ `chunked` (or `failed`)

#### `pipeline_b_chunks_raw`
Parsed chunks from Landing AI, before embedding generation.

```sql
- id (UUID, PK)
- document_id (UUID, FK â†’ pipeline_b_documents)
- content (TEXT) -- Actual chunk text
- chunk_type ('text' | 'table' | 'list' | 'code_block' | 'header')
- chunk_index (INT) -- Position in document
- page_number (INT, nullable)
- visual_grounding (JSONB) -- Bounding boxes from Landing AI
- embedding (vector(1536)) -- OpenAI embedding
- embedding_status ('pending' | 'processing' | 'ready' | 'failed')
- embedding_error (TEXT)
- created_at, embedded_at (TIMESTAMPTZ)
```

**Embedding Status Flow**: `pending` â†’ `processing` â†’ `ready` (or `failed`)

#### `pipeline_b_agent_knowledge`
Links agents to specific chunks they can access.

```sql
- id (UUID, PK)
- agent_id (UUID, FK â†’ agents)
- chunk_id (UUID, FK â†’ pipeline_b_chunks_raw)
- synced_at (TIMESTAMPTZ)
- is_active (BOOLEAN)
- UNIQUE(agent_id, chunk_id)
```

## Edge Functions

### 1. `pipeline-b-ingest-pdf`
**Purpose**: Immediate PDF upload  
**Auth**: Required (`verify_jwt = true`)  
**Timeout**: Default (10s)

**Input**:
```json
FormData: { file: File }
```

**Output**:
```json
{
  "success": true,
  "documentId": "uuid",
  "fileName": "document.pdf",
  "status": "ingested",
  "message": "PDF uploaded successfully. Processing will begin automatically."
}
```

**What it does**:
1. Upload file to `knowledge-pdfs` storage bucket
2. Create record in `pipeline_b_documents` with `status='ingested'`
3. Return immediately (no processing)

**Duration**: ~20-50ms

---

### 2. `pipeline-b-ingest-github`
**Purpose**: Import files from GitHub repository  
**Auth**: Required (`verify_jwt = true`)  
**Timeout**: Default (10s)

**Input**:
```json
{
  "repoUrl": "https://github.com/org/repo",
  "branch": "main",
  "filePaths": [
    "README.md",
    "src/index.ts",
    "docs/guide.md"
  ]
}
```

**Output**:
```json
{
  "success": true,
  "documentsIngested": 3,
  "documentIds": ["uuid1", "uuid2", "uuid3"],
  "message": "GitHub files ingested successfully. Processing will begin automatically."
}
```

**What it does**:
1. Fetch each file from GitHub raw URL
2. Store `full_text` in `pipeline_b_documents`
3. Set `status='ingested'`, `source_type='github'`
4. Return immediately

**Duration**: ~500ms-2s (depending on file count)

---

### 3. `pipeline-b-process-chunks` â° CRON
**Purpose**: Parse documents with Landing AI  
**Auth**: None (`verify_jwt = false`)  
**Timeout**: 600s  
**Schedule**: Every 10 minutes (`*/10 * * * *`)

**Input**: None (auto-triggered)

**Output**:
```json
{
  "processed": 3,
  "failed": 1,
  "errors": [
    { "id": "uuid", "error": "Landing AI timeout" }
  ]
}
```

**What it does**:
1. Fetch up to 5 documents with `status='ingested'`
2. For each document:
   - Mark as `processing`
   - Download file (PDFs) or read `full_text` (GitHub)
   - Call Landing AI parse API
   - Store returned chunks in `pipeline_b_chunks_raw`
   - Mark document as `chunked`
3. On failure: mark as `failed` with `error_message`

**Duration**: ~30s-5min per document (depends on size)

---

### 4. `pipeline-b-generate-embeddings` â° CRON
**Purpose**: Generate OpenAI embeddings for chunks  
**Auth**: None (`verify_jwt = false`)  
**Timeout**: 600s  
**Schedule**: Every 5 minutes (`*/5 * * * *`)

**Input**: None (auto-triggered)

**Output**:
```json
{
  "processed": 48,
  "failed": 2,
  "errors": [
    { "id": "chunk-uuid", "error": "OpenAI rate limit" }
  ]
}
```

**What it does**:
1. Fetch up to 50 chunks with `embedding_status='pending'`
2. For each chunk:
   - Mark as `processing`
   - Call OpenAI `text-embedding-3-small`
   - Validate embedding (1536 dimensions)
   - Store in `pipeline_b_chunks_raw.embedding`
   - Mark as `ready`
3. Rate limiting: 100ms delay between requests
4. On failure: mark as `failed` with `embedding_error`

**Duration**: ~5-30s per batch

---

### 5. `pipeline-b-sync-agent`
**Purpose**: Sync chunks to specific agent  
**Auth**: Required (`verify_jwt = true`)  
**Timeout**: Default (10s)

**Input**:
```json
{
  "agentId": "agent-uuid",
  "documentIds": ["doc-uuid-1", "doc-uuid-2"] // optional
}
```

**Output**:
```json
{
  "success": true,
  "synced": 145,
  "documentsProcessed": 3,
  "totalChunks": 145,
  "agent": {
    "id": "agent-uuid",
    "name": "CupidGPT"
  }
}
```

**What it does**:
1. Verify agent exists
2. Fetch chunks with `embedding_status='ready'`
3. Filter by `documentIds` if provided
4. Insert into `pipeline_b_agent_knowledge` (upsert)
5. Return sync stats

**Duration**: ~100ms-2s

---

## Usage Examples

### Frontend: Upload PDF

```typescript
const formData = new FormData();
formData.append('file', pdfFile);

const { data } = await supabase.functions.invoke('pipeline-b-ingest-pdf', {
  body: formData,
});

console.log(data.documentId); // Save for later sync
```

### Frontend: Import from GitHub

```typescript
const { data } = await supabase.functions.invoke('pipeline-b-ingest-github', {
  body: {
    repoUrl: 'https://github.com/airtop-ai/examples-typescript',
    branch: 'main',
    filePaths: [
      'packages/basic-browser-control/README.md',
      'packages/basic-browser-control/src/index.ts',
    ]
  }
});

console.log(data.documentIds); // Save for agent sync
```

### Frontend: Sync to Agent

```typescript
const { data } = await supabase.functions.invoke('pipeline-b-sync-agent', {
  body: {
    agentId: 'agent-123',
    documentIds: ['doc-456', 'doc-789'], // optional
  }
});

console.log(`Synced ${data.synced} chunks to ${data.agent.name}`);
```

### Frontend: Monitor Processing Status

```typescript
// Poll document status
const { data: doc } = await supabase
  .from('pipeline_b_documents')
  .select('id, file_name, status, error_message')
  .eq('id', documentId)
  .single();

// Check chunk status
const { data: chunks } = await supabase
  .from('pipeline_b_chunks_raw')
  .select('embedding_status')
  .eq('document_id', documentId);

const ready = chunks.filter(c => c.embedding_status === 'ready').length;
const total = chunks.length;

console.log(`${ready}/${total} chunks ready`);
```

---

## Monitoring & Observability

### Key Queries

**Check pipeline health:**
```sql
SELECT 
  status,
  COUNT(*) as count,
  AVG(EXTRACT(EPOCH FROM (processed_at - created_at))) as avg_processing_seconds
FROM pipeline_b_documents
GROUP BY status;
```

**Check embedding backlog:**
```sql
SELECT 
  embedding_status,
  COUNT(*) as count
FROM pipeline_b_chunks_raw
GROUP BY embedding_status;
```

**Check agent sync status:**
```sql
SELECT 
  a.name,
  COUNT(DISTINCT pak.chunk_id) as chunk_count,
  COUNT(DISTINCT c.document_id) as document_count
FROM agents a
LEFT JOIN pipeline_b_agent_knowledge pak ON pak.agent_id = a.id
LEFT JOIN pipeline_b_chunks_raw c ON c.id = pak.chunk_id
GROUP BY a.id, a.name;
```

---

## Troubleshooting

### Issue: Documents stuck in "ingested"

**Cause**: `pipeline-b-process-chunks` not running or erroring

**Check**:
```sql
SELECT * FROM pipeline_b_documents
WHERE status = 'ingested'
ORDER BY created_at ASC
LIMIT 10;
```

**Fix**: Manually trigger function or check logs

---

### Issue: Chunks stuck in "pending"

**Cause**: `pipeline-b-generate-embeddings` failing or rate limited

**Check**:
```sql
SELECT 
  embedding_status,
  embedding_error,
  COUNT(*) 
FROM pipeline_b_chunks_raw
WHERE embedding_status IN ('pending', 'failed')
GROUP BY embedding_status, embedding_error;
```

**Fix**: Check OpenAI API key, rate limits, or manually reprocess

---

### Issue: Agent sees no knowledge

**Cause**: Documents not synced to agent

**Check**:
```sql
SELECT COUNT(*) FROM pipeline_b_agent_knowledge
WHERE agent_id = 'your-agent-id';
```

**Fix**: Call `pipeline-b-sync-agent` with correct `agentId`

---

## Migration from Old System

Pipeline B is **completely independent**. No migration needed. You can:

1. **Run both systems in parallel** during testing
2. **Gradually switch agents** from old to new system
3. **Test with new agents first** before migrating existing ones
4. **Delete old tables** once confident in Pipeline B

**No conflicts. No Frankenstein code.**

---

## Performance Characteristics

| Stage | Latency | Throughput | Bottleneck |
|-------|---------|------------|------------|
| Ingest PDF | ~50ms | Unlimited | Storage I/O |
| Ingest GitHub | ~500ms-2s | 10 files/s | GitHub API |
| Process Chunks | ~1-5min/doc | 5 docs/10min | Landing AI API |
| Generate Embeddings | ~10s/50chunks | 50 chunks/5min | OpenAI rate limits |
| Sync Agent | ~100ms | Unlimited | Database I/O |

**Total time** from upload to agent-ready:
- **Minimum**: ~5 minutes (small document, light load)
- **Average**: ~15-30 minutes (typical document)
- **Maximum**: ~2 hours (large document, heavy load)

---

## Security

- All tables have RLS enabled
- Authenticated users can CRUD their own documents
- Agents can only access chunks explicitly synced to them
- Cron functions run with service role (no JWT)
- User-facing functions require authentication

---

## Cost Estimation

**Per 1000 documents** (average 50 pages each):

- **Landing AI**: ~$50-100 (depends on plan)
- **OpenAI Embeddings**: ~$20 (1536-dim model)
- **Supabase Storage**: ~$0.50/GB
- **Supabase Database**: ~$5/month (included in plan)

**Total**: ~$75-125 per 1000 documents

---

## Next Steps

1. âœ… **Deploy functions** (automatic)
2. â³ **Set up cron jobs** (see config.toml)
3. ğŸ§ª **Test with small PDF**
4. ğŸ“Š **Monitor processing pipeline**
5. ğŸš€ **Sync to first agent**
6. ğŸ¯ **Scale to production**

---

**Pipeline B is ready. Zero Frankenstein code. 100% clean architecture.**
