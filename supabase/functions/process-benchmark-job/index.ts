import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2";

const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Headers': 'authorization, x-client-info, apikey, content-type',
};

const BENCHMARK_AGENT_SLUG = 'book-serach-expert';
const BENCHMARK_USER_ID = '00000000-0000-0000-0000-000000000001';
const BATCH_SIZE = 5; // Process up to 5 jobs per invocation

serve(async (req) => {
  if (req.method === 'OPTIONS') {
    return new Response(null, { headers: corsHeaders });
  }

  const supabaseUrl = Deno.env.get('SUPABASE_URL')!;
  const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')!;
  const supabase = createClient(supabaseUrl, supabaseServiceKey);

  try {
    const body = await req.json().catch(() => ({}));
    const { job_id, fallback_mode } = body;
    
    // Single job mode (explicit job_id provided)
    if (job_id && !fallback_mode) {
      console.log(`[Process Benchmark Job] Single job mode: ${job_id}`);
      const result = await processJob(supabase, supabaseUrl, supabaseServiceKey, job_id);
      return new Response(JSON.stringify(result), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' }
      });
    }
    
    // ========== ZOMBIE JOB RECOVERY ==========
    // Reset jobs stuck in 'processing' for more than 5 minutes (likely timeout/crash)
    const STUCK_THRESHOLD_MINUTES = 5;
    const stuckCutoff = new Date(Date.now() - STUCK_THRESHOLD_MINUTES * 60 * 1000).toISOString();
    
    const { data: stuckJobs } = await supabase
      .from('benchmark_jobs_queue')
      .select('id, attempts, max_attempts, started_at')
      .eq('status', 'processing')
      .lt('started_at', stuckCutoff);
    
    if (stuckJobs && stuckJobs.length > 0) {
      console.log(`[Process Benchmark Job] ðŸ§Ÿ Found ${stuckJobs.length} zombie jobs stuck in processing`);
      
      for (const stuckJob of stuckJobs) {
        if (stuckJob.attempts >= stuckJob.max_attempts) {
          // Max retries exceeded - mark as failed
          await supabase
            .from('benchmark_jobs_queue')
            .update({
              status: 'failed',
              completed_at: new Date().toISOString(),
              error_message: `Exceeded max attempts (${stuckJob.max_attempts}) - job kept timing out`
            })
            .eq('id', stuckJob.id);
          console.log(`[Process Benchmark Job] âŒ Zombie job ${stuckJob.id} marked failed (max retries exceeded)`);
        } else {
          // Reset to pending for retry
          await supabase
            .from('benchmark_jobs_queue')
            .update({
              status: 'pending',
              started_at: null,
              error_message: `Auto-reset from stuck processing state (attempt ${stuckJob.attempts})`
            })
            .eq('id', stuckJob.id);
          console.log(`[Process Benchmark Job] ðŸ”„ Zombie job ${stuckJob.id} reset to pending (attempt ${stuckJob.attempts}/${stuckJob.max_attempts})`);
        }
      }
    }
    
    // ========== BATCH MODE: pick up to BATCH_SIZE pending jobs ==========
    const { data: pendingJobs } = await supabase
      .from('benchmark_jobs_queue')
      .select('id')
      .eq('status', 'pending')
      .order('created_at', { ascending: true })
      .limit(BATCH_SIZE);
    
    if (!pendingJobs || pendingJobs.length === 0) {
      return new Response(JSON.stringify({ 
        skipped: true, 
        reason: 'No pending jobs',
        zombies_recovered: stuckJobs?.length || 0
      }), {
        headers: { ...corsHeaders, 'Content-Type': 'application/json' }
      });
    }
    
    console.log(`[Process Benchmark Job] Batch mode: processing ${pendingJobs.length} jobs`);
    
    // Process each job sequentially within this invocation
    const results: any[] = [];
    for (const job of pendingJobs) {
      try {
        const result = await processJob(supabase, supabaseUrl, supabaseServiceKey, job.id);
        results.push({ job_id: job.id, ...result });
      } catch (jobError) {
        console.error(`[Process Benchmark Job] Error processing job ${job.id}:`, jobError);
        results.push({ job_id: job.id, error: String(jobError) });
      }
    }
    
    const completed = results.filter(r => r.success).length;
    const failed = results.filter(r => r.error).length;
    console.log(`[Process Benchmark Job] Batch complete: ${completed} success, ${failed} failed`);

    // SELF-CONTINUATION: Check for more pending jobs and trigger next batch
    const { count: remainingCount } = await supabase
      .from('benchmark_jobs_queue')
      .select('*', { count: 'exact', head: true })
      .eq('status', 'pending');

    if (remainingCount && remainingCount > 0) {
      console.log(`[Process Benchmark Job] ðŸ”„ ${remainingCount} jobs remaining - triggering self-continuation`);
      
      const anonKey = Deno.env.get('SUPABASE_ANON_KEY')!;
      
      const triggerNext = async () => {
        try {
          await fetch(`${supabaseUrl}/functions/v1/process-benchmark-job`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${anonKey}`
            },
            body: JSON.stringify({ fallback_mode: true })
          });
        } catch (err) {
          console.error(`[Process Benchmark Job] Self-continuation error:`, err);
        }
      };

      // Fire and forget
      (globalThis as any).EdgeRuntime?.waitUntil?.(triggerNext()) || triggerNext();
    } else {
      console.log(`[Process Benchmark Job] âœ… All jobs processed - no continuation needed`);
    }

    return new Response(JSON.stringify({
      batch: true,
      processed: results.length,
      completed,
      failed,
      remaining: remainingCount || 0,
      results
    }), {
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });

  } catch (error) {
    console.error(`[Process Benchmark Job] Critical error:`, error);
    return new Response(JSON.stringify({ 
      error: error instanceof Error ? error.message : 'Unknown error' 
    }), {
      status: 500,
      headers: { ...corsHeaders, 'Content-Type': 'application/json' }
    });
  }
});

async function processJob(
  supabase: any, 
  supabaseUrl: string, 
  supabaseServiceKey: string, 
  targetJobId: string
): Promise<any> {
  console.log(`[Process Benchmark Job] Starting job: ${targetJobId}`);

  // Step 1: Fetch job with pessimistic locking pattern
  const { data: job, error: fetchError } = await supabase
    .from('benchmark_jobs_queue')
    .select('*, benchmark_datasets(*)')
    .eq('id', targetJobId)
    .eq('status', 'pending')
    .single();

  if (fetchError || !job) {
    console.log(`[Process Benchmark Job] Job ${targetJobId} not found or not pending - skipping`);
    return { skipped: true, reason: 'Job not pending or not found' };
  }

  // Step 2: Mark as processing with incremented attempts
  const { error: updateError } = await supabase
    .from('benchmark_jobs_queue')
    .update({
      status: 'processing',
      started_at: new Date().toISOString(),
      attempts: job.attempts + 1
    })
    .eq('id', targetJobId)
    .eq('status', 'pending'); // Double-check still pending

  if (updateError) {
    console.error(`[Process Benchmark Job] Failed to mark job as processing:`, updateError);
    return { error: 'Failed to acquire job lock' };
  }

  const question = job.benchmark_datasets;
  if (!question) {
    await markJobFailed(supabase, targetJobId, 'Question data not found');
    return { error: 'Question not found' };
  }

  console.log(`[Process Benchmark Job] Processing: "${question.question.substring(0, 50)}..."`);

  // Step 3: Generate unique conversation ID
  const conversationId = crypto.randomUUID();

  // Step 4: Call agent-chat with extended timeout
  const startTime = Date.now();
  let agentResponse = '';
  let responseTimeMs = 0;

  try {
    const controller = new AbortController();
    const AGENT_TIMEOUT_MS = 55_000; // 55s - safely under edge function 60s limit
    const timeoutId = setTimeout(() => controller.abort(), AGENT_TIMEOUT_MS);

    const prefixedQuestion = `Regarding document '${question.file_name}': ${question.question}`;
    
    const agentChatResponse = await fetch(`${supabaseUrl}/functions/v1/agent-chat`, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${supabaseServiceKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        agentSlug: BENCHMARK_AGENT_SLUG,
        message: prefixedQuestion,
        conversationId,
        stream: false,
        serverUserId: BENCHMARK_USER_ID,
        documentFilter: question.file_name  // EXPLICIT PRE-FILTER: ensures semantic search is restricted to this document
      }),
      signal: controller.signal
    });

    clearTimeout(timeoutId);
    responseTimeMs = Date.now() - startTime;

    if (!agentChatResponse.ok) {
      const errorText = await agentChatResponse.text();
      throw new Error(`Agent chat failed: ${agentChatResponse.status} - ${errorText}`);
    }

    // Parse JSON response (stream: false returns JSON, not SSE)
    const responseData = await agentChatResponse.json();
    
    if (responseData.response) {
      agentResponse = responseData.response;
    } else if (responseData.error) {
      throw new Error(`Agent error: ${responseData.error}`);
    }

    if (!agentResponse.trim()) {
      throw new Error('Empty response from agent');
    }

    console.log(`[Process Benchmark Job] Got response (${agentResponse.length} chars) in ${responseTimeMs}ms`);

  } catch (agentError) {
    const errorMessage = agentError instanceof Error ? agentError.message : 'Unknown agent error';
    console.error(`[Process Benchmark Job] Agent error:`, errorMessage);
    
    // Check if we should retry
    if (job.attempts + 1 < job.max_attempts) {
      await supabase
        .from('benchmark_jobs_queue')
        .update({
          status: 'pending',
          error_message: errorMessage
        })
        .eq('id', targetJobId);
      
      console.log(`[Process Benchmark Job] Job will retry (attempt ${job.attempts + 1}/${job.max_attempts})`);
      return { retry: true, attempt: job.attempts + 1 };
    }
    
    await markJobFailed(supabase, targetJobId, errorMessage);
    return { error: errorMessage };
  }

  // Step 5: Evaluate answer
  let evaluationResult = { correct: false, reason: 'Evaluation failed' };
  
  try {
    const evalResponse = await supabase.functions.invoke('evaluate-answer', {
      body: {
        question: question.question,
        agentResponse,
        groundTruths: [question.ground_truth],
        suiteCategory: question.suite_category
      }
    });

    if (evalResponse.data) {
      evaluationResult = evalResponse.data;
    }
  } catch (evalError) {
    console.error(`[Process Benchmark Job] Evaluation error:`, evalError);
    evaluationResult = { correct: false, reason: 'Evaluation failed: ' + String(evalError) };
  }

  // Step 6: Save to benchmark_results
  await supabase.from('benchmark_results').insert({
    run_id: job.run_id,
    question: question.question,
    ground_truth: question.ground_truth,
    agent_response: agentResponse,
    correct: evaluationResult.correct,
    reason: evaluationResult.reason,
    response_time_ms: responseTimeMs,
    status: 'completed',
    pdf_file: question.file_name,
    suite_category: question.suite_category
  });

  // Step 7: Mark job completed
  await supabase
    .from('benchmark_jobs_queue')
    .update({
      status: 'completed',
      completed_at: new Date().toISOString(),
      result: {
        correct: evaluationResult.correct,
        reason: evaluationResult.reason,
        response_time_ms: responseTimeMs
      }
    })
    .eq('id', targetJobId);

  console.log(`[Process Benchmark Job] âœ… Job completed: ${evaluationResult.correct ? 'CORRECT' : 'INCORRECT'}`);

  return {
    success: true,
    job_id: targetJobId,
    correct: evaluationResult.correct,
    response_time_ms: responseTimeMs
  };
}

async function markJobFailed(supabase: any, jobId: string, errorMessage: string) {
  await supabase
    .from('benchmark_jobs_queue')
    .update({
      status: 'failed',
      completed_at: new Date().toISOString(),
      error_message: errorMessage
    })
    .eq('id', jobId);
}
